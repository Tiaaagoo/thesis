\section{Ferramentas de Processamento de Linguagem Natural}
\label{sec:chap03_existingtools}
Atualmente, são disponibilizadas várias ferramentas para lidar com o \gls{pln}, sendo que se torna necessário destacar as mais relevantes e perceber qual pode ser a mais adequada na resolução do problema , dadas as particularidades de cada uma. Posteriormente, faz-se o comparativo de todas as ferramentas, selecionando aquela que se revela ser a mais apropriada, usando uma matriz de decisão para o efeito.

\subsection{NLTK}
O NLTK, \textit{Natural Language Toolkit}, foi criado em 2001 na Universidade da Pennsylvania e, desde então, tem-se expandido graças à comunidade \textit{open source} que contribui para o projecto~\parencite{applied_natural_language_processing_with_python}. Esta biblioteca, desenvolvida em \textit{Python} para processamento de linguagem natural e análise textual, caracteriza-se pela sua simplicidade, consistência, extensibilidade e modularidade, apresentando um conjunto de funções otimizadas para suportar estas tarefas listadas~\parencite{applied_natural_language_processing_with_python, python_text_processing_nltk_cookbook}. De acordo com~\textcite{nltk_education_scientific_purposes}, a combinação de \textit{Python} e NLTK dá a capacidade a qualquer programador de resolver facilmente tarefas de \gls{pln}, evitando demasiado tempo a  estudar os conceitos inerentes. Neste contexto, a biblioteca está integrada com \textit{WordNet}\footnote{Disponível em \url{https://wordnet.princeton.edu/}}, uma base de dados de relações semânticas entre nomes, verbos, adjetivos e advérbios da Língua Inglesa~\parencite{nltk_education_scientific_purposes}.

O NLTK providencia poderosas ferramentas estatísticas, está preparado para trabalhar com grandes \textit{datasets}, criar modelos linguísticos robustos, possibilitando a sua extensão para componentes que possam ser usados em sistemas produtivos~\parencite{nltk_education_scientific_purposes, applied_natural_language_processing_with_python}.

\subsection{Stanford CoreNLP}
O Stanford CoreNLP é uma \textit{framework} que providencia um conjunto de ferramentas para analisar discurso, reconhecer entidades, normalizar datas, identificar a estrutura frásica e dependência sintática dos termos, entre outras~\parencite{stanford_open_nlp}. Ele possui uma \gls{api} rica, sendo assim acessível em múltiplas linguagens de programação e é a biblioteca mais usada em projetos de pesquisa cuja temática é o \gls{pln}~\parencite{stanford_open_nlp, choosing_nlp_library}.

\subsection{spaCy}
O spaCy é uma biblioteca para métodos avançados de processamento de linguagem natural, desenvolvida com \textit{Python} e \textit{CPython}, e cujo objetivo é suportar a conceção de aplicações de foro comercial~\parencite{choosing_nlp_library}. Esta biblioteca suporta as funcionalidades de \gls{pln} a partir de modelos estatísticos pré-treinados especificamente para o spaCy, tornando-o rápido e preciso~\parencite{spacy_usage}.

\subsection{TensorFlow}
O projeto Google Brain\footnote{Disponível em \url{https://ai.google/research/teams/brain/}} começou, em 2011, com o objetivo de explorar redes neuronais de larga escala, quer para pesquisa, quer para uso nos produtos da Google~\parencite{tensorflow_largescale_machine_learning_distributed_systems}. O TensorFlow é um sistema de segunda geração, sucessor do DistBelief\footnote{Disponível em \url{https://ai.google/research/pubs/pub40565}}, usado para a implementação e implantação de modelos de \gls{ml} de elevada escala~\parencite{tensorflow_largescale_machine_learning_distributed_systems}. Este usa grafos \textit{dataflow}, um modelo de grafo que expressa as possibilidades de execução concorrente de partes de um programa, para representar computacionalmente o estado partilhado e as operações responsáveis pela mutação desse estado, o que inclui operações matemáticas individuais, os respetivos parâmetros, as suas regras de atualização e o pré-processamento dos dados de entrada~\parencite{data_flow_graphs_encyclopedia_parellel_computing, tensorflow_system_largescale_machine_learning}.

Num contexto de \gls{pln}, usando TensorFlow, pode aplicar-se redes neuronais convolucionais para tarefas de classificação, como análise de sentimento, deteção de \textit{spam} ou categorização de tópicos~\parencite{understanding_convolution_neural_networks_nlp}. Embora as redes neuronais convolucionais sejam tipicamente usadas na identificação de imagens, podem também ser usadas em tarefas \gls{pln}, usando as palavras como entrada, ao invés dos pixeis, sendo a sua computação rápida e eficiente~\parencite{understanding_convolution_neural_networks_nlp}.

\subsection{Amazon Lex}
O Lex é um produto do \textit{Amazon Web Services} (mais conhecida como AWS), uma plataforma de serviços da Amazon na \textit{cloud}, para construir interfaces de conversação integradas com qualquer aplicação, usando voz ou texto, fornecendo funcionalidades de reconhecimento automático de discurso, respetiva conversão para texto e compreensão de linguagem natural, o que possibilita a identificação das intenções manifestadas pelo utilizador, duma forma simples de usar, implantar e escalar~\parencite{amazon_lex_official}. Como demonstrado em~\textcite{aws_ml_blog_conversational_business}, o Amazon Lex garante a sua customização por forma a desenvolver um \textit{chatbot} capaz de interagir com o utilizador, convertendo os seus pedidos de linguagem natural para \gls{sql}, obtendo os dados de uma base de dados relacional, e por fim, apresentá-los.

\subsection{IBM Watson Assistant}
O Watson Assistant é um produto do IBM Watson, um plataforma de serviços na \textit{cloud}, orientados à inteligência artificial e disponibilizado pela IBM~\parencite{ibm_watson_official}. Este produto oferece uma interface de conversação que pode ser integrada em qualquer aplicação ou dispositivo, e tal como os seus concorrentes, destaca-se por usar técnicas avançadas de compreensão de linguagem natural (suportando intenções e entidades), por ser simples de implantar e fácil de usar~\parencite{ibm_watson_assistant_official}. O IBM Watson Assistant também permite a conceção de \textit{chatbots} capazes de interpretar o pedido do utilizador e extrair o conteúdo desejado de uma base de dados~\parencite{ibm_watson_assistant_database_driven_chatbot}.

\subsection{Microsoft LUIS}
O LUIS, acrónimo para \textit{Language Understanding Intelligent Service}, é um serviço da Microsoft, baseado na \textit{cloud} e parte do produto \textit{Cognitive Services}, e que por sua vez integra na plataforma Azure, que aplica métodos de \gls{ml} à linguagem natural, permitindo obter informação relevante acerca da mesma~\parencite{microsoft_luis_official}. O intuito do LUIS é tornar possível a criação de soluções que apliquem modelos específicos de \gls{ml} para a compreensão de linguagem específica de um determinado domínio, sem qualquer tipo de perícia nesta área~\parencite{luis_fast_easy_language_understanding}. Assim, o LUIS permite a construção do próprio modelo, através da identificação de entidades, de intenções e das frases que lhes são associadas, oferecendo diversas ferramentas ao desenvolvedor para que este processo seja mais simples~\parencite{microsoft_luis_official}. Relativamente à sua aplicabilidade ao problema, a combinação do LUIS com o QnA Maker (outro serviço integrante do \textit{Cognitive Services}) resulta numa solução capaz de identificar as intenções e entidades, entregando respostas pré-fabricadas ou informação de uma determinada fonte~\parencite{microsoft_luis_use_nl_processing_service}.

\subsection{Sumário}
As ferramentas apresentadas previamente podem ser divididas por tipo; as quatro primeiras referem-se a bibliotecas ou \textit{frameworks} -- NLTK, Stanford CoreNLP, spaCy e TensorFlow --, as últimas três dizem respeito a serviços disponibilizados na \textit{cloud} - Amazon Lex, IBM Watson Assistant e Microsoft LUIS. Por essa razão, sendo que o objetivo é perceber qual a ferramenta mais adequada, é importante notar que a natureza de cada uma é diferente, o que influencia o comparativo entre elas. Portanto, essa comparação não deve ser feita pelas funcionalidades que cada uma oferece, mas sim pelo o valor que poderá trazer à solução, ou seja, pelos critérios e/ou restrições que cumprem.

Assim sendo, deve-se indicar o conjunto de critérios mais relevantes, alguns já mencionados na Secção~\ref{sec:chap02_valueanalysis}, no tópico de geração, enriquecimento e seleção de ideias para o módulo. Para auxiliar no processo de decisão, opta-se por se usar uma matriz de decisão, uma ferramenta que permite identificar rapidamente a melhor alternativa, a partir de um conjunto pré-definido de critérios e da escala de avaliação~\parencite{decisions_engineering_design}.
Para os critérios de avaliação consideram-se que têm todos o mesmo grau de importância, pelo que são descritos a seguir.

\begin{itemize}
    \item
    {
        \textit{Dependência} -- a nível de plataforma, linguagem de programação, ambiente de desenvolvimento, acesso à Internet ou outras;
    }
    \item
    {
        \textit{Custo} -- possível preço da solução e/ou gastos com recursos;  
    }
    \item
    {
        \textit{Facilidade} -- utilização na perspetiva do programador, ou seja, se apresenta boa documentação, fácil e rápido de aprender e desenvolver;
    }
    \item
    {
        \textit{Esforço} -- dispêndio no desenvolvimento da solução. Ferramentas com modelos pré-treinados necessitarão de menos esforço do que ferramentas que não os forneçam;
    }
    \item
    {
        \textit{Escalabilidade} -- como é que a ferramenta se comporta quando a aplicação tem a necessidade de escalar, ou seja, se o nível de eficiência ou a facilidade de desenvolvimento se mantém constante.
    }
\end{itemize}

Quanto à escala de avaliação, opta-se por três níveis: um ($1$) até três ($3$), para indicar a baixa e elevada adequabilidade, respetivamente. Por exemplo, se para o critério \textit{Custo} a pontuação dada é um ($1$), significa que o item/ferramenta, nesse critério, é pouco adequado para a solução. A soma de todas as pontuações define a adequabilidade da ferramenta. Quanto mais alta for, mais adequada é. De salientar que a avaliação é feita com base na bibliografia recolhida para as ferramentas estudadas. Na Tabela~\ref{tab:tools_comparison} apresenta-se a matriz de decisão para as ferramentas estudadas anteriormente.

\begin{table}[!ht]
\caption{Comparativo das ferramentas de processamento de linguagem natural}
\label{tab:tools_comparison}
\centering
\resizebox{\textwidth}{!}{\input{ch03/assets/tools_comparison.tex}}
\end{table}

A matriz de decisão apresentada na tabela anterior aponta o spaCy como a ferramenta mais adequada para o módulo de linguagem natural, pois cumpre a maioria dos critérios e não se trata de uma ferramenta na \textit{cloud}, visto que esta última é uma restrição da empresa. Em suma, tanto o spaCy como o TensorFlow são bons candidatos para o desenvolvimento da solução final, embora possam existir ferramentas mais adequadas no mercado, que são desconhecidas, e por isso, não contempladas no estudo. Para o contexto desta tese, como prova de conceito, o uso de uma ferramenta na \textit{cloud} é aceitável -- o Microsoft LUIS, por exemplo --, visto que se pretende validar uma abordagem.
